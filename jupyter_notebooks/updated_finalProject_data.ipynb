{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da3de2e2",
   "metadata": {},
   "source": [
    "###  CS439 Final Project - TDA + ML using USDA Food Environment Atlas (2019) + Census Tract list (2025)\n",
    "Denise Gonzalez-Cruz,   netid: dg1070,    section: 08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d42e0b3",
   "metadata": {},
   "source": [
    "notes:\n",
    "- This notebook targets: both Census Tract list (2025) + USDA Food Environment Atlas (2019), with possible ACS (American Community Survey) integration\n",
    "- Mapping support: both TIGER/Line shapefile (.shp) AND GeoJSON are supported (we will try GeoJSON first if available).\n",
    "- TDA pipeline: GUDHI (unweighted + weighted Rips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f831f860",
   "metadata": {},
   "source": [
    "Final notes:\n",
    "- This notebook focuses on GUDHI-only, weighted-first VR complexes using weighting focused on socioeconomic features\n",
    "- The slowest step is computing local per-tract diagrams. We can consider subsampling or increasing k for speed.\n",
    "- can easily change alpha_poverty/alpha_income/alpha_density at the top to experiment with different outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6923f882",
   "metadata": {},
   "source": [
    "Overall, this code had to be redone as the original data I was using was the ACS ZTCA which groups areas together instead of keeping the data at the census tract level. I was able to find and properly merge the data in the NJ and other jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f7ff177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) imports\n",
    "import os, time, warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# TDA: GUDHI\n",
    "USE_GUDHI = True\n",
    "try:\n",
    "    import gudhi as gd\n",
    "    import gudhi.representations as gdr\n",
    "except Exception as e:\n",
    "    USE_GUDHI = False\n",
    "    warnings.warn(f'GUDHI import failed: {e}. Install GUDHI to run the TDA sections.')\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68f46409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>AGRITRSM_OPS12</th>\n",
       "      <th>AGRITRSM_OPS17</th>\n",
       "      <th>AGRITRSM_RCT12</th>\n",
       "      <th>AGRITRSM_RCT17</th>\n",
       "      <th>BERRY_ACRES12</th>\n",
       "      <th>BERRY_ACRES17</th>\n",
       "      <th>BERRY_ACRESPTH12</th>\n",
       "      <th>...</th>\n",
       "      <th>VEG_ACRESPTH12</th>\n",
       "      <th>VEG_ACRESPTH17</th>\n",
       "      <th>VEG_FARMS12</th>\n",
       "      <th>VEG_FARMS17</th>\n",
       "      <th>VLFOODSEC_18_20</th>\n",
       "      <th>VLFOODSEC_21_23</th>\n",
       "      <th>WICS16</th>\n",
       "      <th>WICS22</th>\n",
       "      <th>WICSPTH16</th>\n",
       "      <th>WICSPTH22</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Autauga</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>146000.0</td>\n",
       "      <td>66000.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.090269</td>\n",
       "      <td>...</td>\n",
       "      <td>22.206174</td>\n",
       "      <td>21.808991</td>\n",
       "      <td>45.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.090828</td>\n",
       "      <td>0.068072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1003.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Baldwin</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>204000.0</td>\n",
       "      <td>337000.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>0.437604</td>\n",
       "      <td>...</td>\n",
       "      <td>9.213207</td>\n",
       "      <td>7.773349</td>\n",
       "      <td>50.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.145356</td>\n",
       "      <td>0.115671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1005.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Barbour</td>\n",
       "      <td>32.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>304000.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.669515</td>\n",
       "      <td>...</td>\n",
       "      <td>1.629765</td>\n",
       "      <td>2.424772</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.338168</td>\n",
       "      <td>0.361780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1007.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Bibb</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>21000.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>-9999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.620843</td>\n",
       "      <td>0.221729</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.221513</td>\n",
       "      <td>0.179767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>Blount</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>0.657587</td>\n",
       "      <td>...</td>\n",
       "      <td>11.715438</td>\n",
       "      <td>10.002250</td>\n",
       "      <td>64.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.138639</td>\n",
       "      <td>0.084635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3151</th>\n",
       "      <td>9150.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>Northeastern Connecticut</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3152</th>\n",
       "      <td>9160.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>Northwest Hills</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3153</th>\n",
       "      <td>9170.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>South Central Connecticut</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3154</th>\n",
       "      <td>9180.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>Southeastern Connecticut</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3155</th>\n",
       "      <td>9190.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>Western Connecticut</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.000000</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>-8888.0</td>\n",
       "      <td>4.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3156 rows × 307 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FIPS State                     County  AGRITRSM_OPS12  AGRITRSM_OPS17  \\\n",
       "0     1001.0    AL                    Autauga            10.0             6.0   \n",
       "1     1003.0    AL                    Baldwin            16.0            19.0   \n",
       "2     1005.0    AL                    Barbour            32.0            11.0   \n",
       "3     1007.0    AL                       Bibb             6.0             3.0   \n",
       "4     1009.0    AL                     Blount             8.0            13.0   \n",
       "...      ...   ...                        ...             ...             ...   \n",
       "3151  9150.0    CT   Northeastern Connecticut         -8888.0         -8888.0   \n",
       "3152  9160.0    CT            Northwest Hills         -8888.0         -8888.0   \n",
       "3153  9170.0    CT  South Central Connecticut         -8888.0         -8888.0   \n",
       "3154  9180.0    CT   Southeastern Connecticut         -8888.0         -8888.0   \n",
       "3155  9190.0    CT        Western Connecticut         -8888.0         -8888.0   \n",
       "\n",
       "      AGRITRSM_RCT12  AGRITRSM_RCT17  BERRY_ACRES12  BERRY_ACRES17  \\\n",
       "0           146000.0         66000.0            5.0            4.0   \n",
       "1           204000.0        337000.0           93.0          113.0   \n",
       "2           304000.0         71000.0           42.0            6.0   \n",
       "3            21000.0         -9999.0        -9999.0           21.0   \n",
       "4            30000.0         50000.0           38.0           36.0   \n",
       "...              ...             ...            ...            ...   \n",
       "3151         -8888.0         -8888.0        -8888.0        -8888.0   \n",
       "3152         -8888.0         -8888.0        -8888.0        -8888.0   \n",
       "3153         -8888.0         -8888.0        -8888.0        -8888.0   \n",
       "3154         -8888.0         -8888.0        -8888.0        -8888.0   \n",
       "3155         -8888.0         -8888.0        -8888.0        -8888.0   \n",
       "\n",
       "      BERRY_ACRESPTH12  ...  VEG_ACRESPTH12  VEG_ACRESPTH17  VEG_FARMS12  \\\n",
       "0             0.090269  ...       22.206174       21.808991         45.0   \n",
       "1             0.437604  ...        9.213207        7.773349         50.0   \n",
       "2             1.669515  ...        1.629765        2.424772          7.0   \n",
       "3         -9999.000000  ...        0.620843        0.221729         11.0   \n",
       "4             0.657587  ...       11.715438       10.002250         64.0   \n",
       "...                ...  ...             ...             ...          ...   \n",
       "3151      -8888.000000  ...    -8888.000000    -8888.000000      -8888.0   \n",
       "3152      -8888.000000  ...    -8888.000000    -8888.000000      -8888.0   \n",
       "3153      -8888.000000  ...    -8888.000000    -8888.000000      -8888.0   \n",
       "3154      -8888.000000  ...    -8888.000000    -8888.000000      -8888.0   \n",
       "3155      -8888.000000  ...    -8888.000000    -8888.000000      -8888.0   \n",
       "\n",
       "      VEG_FARMS17  VLFOODSEC_18_20  VLFOODSEC_21_23  WICS16  WICS22  \\\n",
       "0            31.0              5.2              4.4     5.0     4.0   \n",
       "1            39.0              5.2              4.4    29.0    27.0   \n",
       "2            18.0              5.2              4.4     9.0     9.0   \n",
       "3             5.0              5.2              4.4     5.0     4.0   \n",
       "4            54.0              5.2              4.4     8.0     5.0   \n",
       "...           ...              ...              ...     ...     ...   \n",
       "3151      -8888.0          -8888.0              4.4     NaN     NaN   \n",
       "3152      -8888.0          -8888.0              4.4     NaN     NaN   \n",
       "3153      -8888.0          -8888.0              4.4     NaN     NaN   \n",
       "3154      -8888.0          -8888.0              4.4     NaN     NaN   \n",
       "3155      -8888.0          -8888.0              4.4     NaN     NaN   \n",
       "\n",
       "      WICSPTH16  WICSPTH22  \n",
       "0      0.090828   0.068072  \n",
       "1      0.145356   0.115671  \n",
       "2      0.338168   0.361780  \n",
       "3      0.221513   0.179767  \n",
       "4      0.138639   0.084635  \n",
       "...         ...        ...  \n",
       "3151        NaN        NaN  \n",
       "3152        NaN        NaN  \n",
       "3153        NaN        NaN  \n",
       "3154        NaN        NaN  \n",
       "3155        NaN        NaN  \n",
       "\n",
       "[3156 rows x 307 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grouping the state and county data by zipcode \n",
    "atlas = pd.read_csv('2025-food-environment-atlas-data/StateAndCountyData.csv')\n",
    "#print(atlas_data)\n",
    "df_wide = atlas.pivot_table(\n",
    "        index=\"FIPS\",\n",
    "        columns=\"Variable_Code\",\n",
    "        values=\"Value\",\n",
    "        aggfunc=\"first\"\n",
    ").reset_index()\n",
    "\n",
    "# Keep FIPS–State–County metadata \n",
    "meta = atlas[[\"FIPS\", \"State\", \"County\"]].drop_duplicates()\n",
    "\n",
    "# Merge to restore those columns\n",
    "df_wide = meta.merge(df_wide, on=\"FIPS\", how=\"left\")\n",
    "\n",
    "# Reorder so these are the first 3 columns\n",
    "first = [\"FIPS\", \"State\", \"County\"]\n",
    "other = [c for c in df_wide.columns if c not in first]\n",
    "\n",
    "df_wide = df_wide[first + other]\n",
    "\n",
    "# changin type of FIPS to str and padding with leading zeros\n",
    "df_wide['FIPS'] = df_wide['FIPS'].apply(lambda x: str(x).zfill(5) if pd.notnull(x) else x)\n",
    "\n",
    "# dropping alaska areas since they don't map to traditional fips codes, and are relatively few in number when it comes to US as a whole\n",
    "# this may affect analysis if we were to focus on alaska, but for now we drop them\n",
    "df_atlas= df_wide.dropna(subset=['FIPS']) # dropping n/a fips entries \n",
    "df_atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd41133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>MSA/MD code type</th>\n",
       "      <th>MSA/MD code</th>\n",
       "      <th>State code</th>\n",
       "      <th>County code</th>\n",
       "      <th>Tract</th>\n",
       "      <th>MSA/MD name</th>\n",
       "      <th>State</th>\n",
       "      <th>County name</th>\n",
       "      <th>FIPS code</th>\n",
       "      <th>MSA/MD MFI</th>\n",
       "      <th>Tract MFI</th>\n",
       "      <th>Tract income percentage</th>\n",
       "      <th>Tract income level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>MSA</td>\n",
       "      <td>10180</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "      <td>30101</td>\n",
       "      <td>ABILENE, TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>CALLAHAN COUNTY                               ...</td>\n",
       "      <td>48059030101</td>\n",
       "      <td>68388</td>\n",
       "      <td>61923.0</td>\n",
       "      <td>90.54</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>MSA</td>\n",
       "      <td>10180</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "      <td>30102</td>\n",
       "      <td>ABILENE, TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>CALLAHAN COUNTY                               ...</td>\n",
       "      <td>48059030102</td>\n",
       "      <td>68388</td>\n",
       "      <td>66132.0</td>\n",
       "      <td>96.70</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>MSA</td>\n",
       "      <td>10180</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "      <td>30200</td>\n",
       "      <td>ABILENE, TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>CALLAHAN COUNTY                               ...</td>\n",
       "      <td>48059030200</td>\n",
       "      <td>68388</td>\n",
       "      <td>59531.0</td>\n",
       "      <td>87.04</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>MSA</td>\n",
       "      <td>10180</td>\n",
       "      <td>48</td>\n",
       "      <td>253</td>\n",
       "      <td>20101</td>\n",
       "      <td>ABILENE, TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>JONES COUNTY                                  ...</td>\n",
       "      <td>48253020101</td>\n",
       "      <td>68388</td>\n",
       "      <td>55179.0</td>\n",
       "      <td>80.68</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>MSA</td>\n",
       "      <td>10180</td>\n",
       "      <td>48</td>\n",
       "      <td>253</td>\n",
       "      <td>20102</td>\n",
       "      <td>ABILENE, TX</td>\n",
       "      <td>TX</td>\n",
       "      <td>JONES COUNTY                                  ...</td>\n",
       "      <td>48253020102</td>\n",
       "      <td>68388</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85524</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>99999</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>960900</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VI</td>\n",
       "      <td>SAINT THOMAS ISLAND                           ...</td>\n",
       "      <td>78030960900</td>\n",
       "      <td>52000</td>\n",
       "      <td>51797.0</td>\n",
       "      <td>99.60</td>\n",
       "      <td>Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85525</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>99999</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>961000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VI</td>\n",
       "      <td>SAINT THOMAS ISLAND                           ...</td>\n",
       "      <td>78030961000</td>\n",
       "      <td>52000</td>\n",
       "      <td>38125.0</td>\n",
       "      <td>73.31</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85526</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>99999</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>961100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VI</td>\n",
       "      <td>SAINT THOMAS ISLAND                           ...</td>\n",
       "      <td>78030961100</td>\n",
       "      <td>52000</td>\n",
       "      <td>38158.0</td>\n",
       "      <td>73.38</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85527</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>99999</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>961200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VI</td>\n",
       "      <td>SAINT THOMAS ISLAND                           ...</td>\n",
       "      <td>78030961200</td>\n",
       "      <td>52000</td>\n",
       "      <td>36188.0</td>\n",
       "      <td>69.59</td>\n",
       "      <td>Moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85528</th>\n",
       "      <td>2024-2025</td>\n",
       "      <td>Non-MSA</td>\n",
       "      <td>99999</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>990000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VI</td>\n",
       "      <td>SAINT THOMAS ISLAND                           ...</td>\n",
       "      <td>78030990000</td>\n",
       "      <td>52000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>85529 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Year MSA/MD code type  MSA/MD code  State code  County code  \\\n",
       "0      2024-2025              MSA        10180          48           59   \n",
       "1      2024-2025              MSA        10180          48           59   \n",
       "2      2024-2025              MSA        10180          48           59   \n",
       "3      2024-2025              MSA        10180          48          253   \n",
       "4      2024-2025              MSA        10180          48          253   \n",
       "...          ...              ...          ...         ...          ...   \n",
       "85524  2024-2025          Non-MSA        99999          78           30   \n",
       "85525  2024-2025          Non-MSA        99999          78           30   \n",
       "85526  2024-2025          Non-MSA        99999          78           30   \n",
       "85527  2024-2025          Non-MSA        99999          78           30   \n",
       "85528  2024-2025          Non-MSA        99999          78           30   \n",
       "\n",
       "        Tract  MSA/MD name State  \\\n",
       "0       30101  ABILENE, TX    TX   \n",
       "1       30102  ABILENE, TX    TX   \n",
       "2       30200  ABILENE, TX    TX   \n",
       "3       20101  ABILENE, TX    TX   \n",
       "4       20102  ABILENE, TX    TX   \n",
       "...       ...          ...   ...   \n",
       "85524  960900          NaN    VI   \n",
       "85525  961000          NaN    VI   \n",
       "85526  961100          NaN    VI   \n",
       "85527  961200          NaN    VI   \n",
       "85528  990000          NaN    VI   \n",
       "\n",
       "                                             County name    FIPS code  \\\n",
       "0      CALLAHAN COUNTY                               ...  48059030101   \n",
       "1      CALLAHAN COUNTY                               ...  48059030102   \n",
       "2      CALLAHAN COUNTY                               ...  48059030200   \n",
       "3      JONES COUNTY                                  ...  48253020101   \n",
       "4      JONES COUNTY                                  ...  48253020102   \n",
       "...                                                  ...          ...   \n",
       "85524  SAINT THOMAS ISLAND                           ...  78030960900   \n",
       "85525  SAINT THOMAS ISLAND                           ...  78030961000   \n",
       "85526  SAINT THOMAS ISLAND                           ...  78030961100   \n",
       "85527  SAINT THOMAS ISLAND                           ...  78030961200   \n",
       "85528  SAINT THOMAS ISLAND                           ...  78030990000   \n",
       "\n",
       "       MSA/MD MFI  Tract MFI  Tract income percentage Tract income level  \n",
       "0           68388    61923.0                    90.54             Middle  \n",
       "1           68388    66132.0                    96.70             Middle  \n",
       "2           68388    59531.0                    87.04             Middle  \n",
       "3           68388    55179.0                    80.68             Middle  \n",
       "4           68388        0.0                     0.00            Unknown  \n",
       "...           ...        ...                      ...                ...  \n",
       "85524       52000    51797.0                    99.60             Middle  \n",
       "85525       52000    38125.0                    73.31           Moderate  \n",
       "85526       52000    38158.0                    73.38           Moderate  \n",
       "85527       52000    36188.0                    69.59           Moderate  \n",
       "85528       52000        NaN                      NaN            Unknown  \n",
       "\n",
       "[85529 rows x 14 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# excel census data to csv - with row 1 as the header\n",
    "# using 2024-2025 tracts \n",
    "census_data = pd.read_excel('CensusTractList2025.xlsx', sheet_name='2024-2025 tracts')\n",
    "\n",
    "census_data.to_csv('census_tracts_2025.csv', index=False)\n",
    "census_df = pd.read_csv('census_tracts_2025.csv', dtype={'GEOID': str})\n",
    "\n",
    "census_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c5185",
   "metadata": {},
   "source": [
    "Parameters: these include weights for the distance metric that will account for socioeconomic factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee971a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) parameters + file paths\n",
    "STATE_POSTAL = 'NJ'\n",
    "TRACT_LIST_CSV = 'census_tracts_2025.csv'   # tract list (contains GEOID or state/county/tract cols - tracts for 2025: State code,County code,Tract)\n",
    "FEA_2019_CSV = 'Food_Environment_Atlas_2019.csv'  # USDA FEA (county-level)\n",
    "ACS_CSV = 'ACSST5Y2023.S1901-Data.csv'                   # ACS tract-level CSV with B01003 (population) and GEOID\n",
    "TIGER_SHP = 'tl_2022_34_tract.shp'               # optional TIGER shapefile for NJ (FIPS=34)\n",
    "GEOJSON_PATH = 'nj_tracts.geojson'               # optional geojson\n",
    "OUTPUT_DIR = 'cs439_finalProject/project_outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# weights for distance metric - used in 10)\n",
    "alpha_poverty = 0.2\n",
    "alpha_income = 0.1\n",
    "alpha_density = 0.1\n",
    "\n",
    "# GUDHI parameters\n",
    "MAX_DIM = 1   # compute H0 and H1 homology classes\n",
    "MAX_EDGE = None  # will be set from weighted distance matrix\n",
    "\n",
    "# neighborhood size for local features\n",
    "K_NEIGHBORS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b3e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) load data files\n",
    "print('Loading tract list...')\n",
    "tracts = pd.read_csv(TRACT_LIST_CSV, dtype=str)\n",
    "tracts['County code'].strip()\n",
    "print('Tracts shape:', tracts.shape)\n",
    "\n",
    "print('Loading Food Environment Atlas (2019)...')\n",
    "fea = pd.read_csv(FEA_2019_CSV, dtype=str)\n",
    "print('FEA shape:', fea.shape)\n",
    "\n",
    "print('Loading ACS 5-year (for population/density)...')\n",
    "acs = pd.read_csv(ACS_CSV, dtype=str)\n",
    "print('ACS shape:', acs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38635edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) normalize/construct GEOID in tract list and ACS\n",
    "print(' Constructing/ensuring GEOID fields...')\n",
    "\n",
    "def zero_pad(x,w):\n",
    "    return str(x).zfill(w)\n",
    "\n",
    "# ensure GEOID in tracts\n",
    "if 'GEOID' not in tracts.columns:\n",
    "    # heuristics\n",
    "    state_col = 'State code'\n",
    "    county_col = 'County code'\n",
    "    tract_col = 'Tract'\n",
    "    if state_col and county_col and tract_col:\n",
    "        tracts['STATEFP'] = tracts[state_col].apply(lambda x: zero_pad(x,2))\n",
    "        tracts['COUNTYFP'] = tracts[county_col].apply(lambda x: zero_pad(x,3))\n",
    "        tracts['TRACTCE'] = tracts[tract_col].apply(lambda x: zero_pad(x,6))\n",
    "        tracts['GEOID'] = tracts['STATEFP'] + tracts['COUNTYFP'] + tracts['TRACTCE']\n",
    "    else:\n",
    "        # try to find any 11-digit column\n",
    "        for c in tracts.columns:\n",
    "            vals = tracts[c].dropna().astype(str)\n",
    "            if vals.str.len().isin([11]).all():\n",
    "                tracts['GEOID'] = vals\n",
    "                break\n",
    "\n",
    "tracts['GEOID'] = tracts['GEOID'].astype(str)\n",
    "tracts['COUNTYFIPS'] = tracts['GEOID'].str[:5]\n",
    "print('Unique state prefixes:', tracts['GEOID'].str[:2].unique())\n",
    "\n",
    "# ensure GEOID in ACS\n",
    "acs_cols = [c.lower() for c in acs.columns]\n",
    "geoid_col = next((c for c in acs.columns if c.lower()=='geoid' or c.lower().endswith('geoid')), None)\n",
    "if geoid_col is None:\n",
    "    raise ValueError('ACS CSV must include a GEOID column for tracts (11-digit).')\n",
    "acs[geoid_col] = acs[geoid_col].astype(str).str.zfill(11)\n",
    "acs.rename(columns={geoid_col:'GEOID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbbb3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) filter to the selected state (NJ) \n",
    "## chose NJ based on our current location and data availabiiity - may be a good start for more urban areas/mixed settings\n",
    "print('Filtering to', STATE_POSTAL)\n",
    "postal_to_fips = {'NJ':'34'}\n",
    "state_fips = postal_to_fips[STATE_POSTAL]\n",
    "tracts = tracts[tracts['GEOID'].str.startswith(state_fips)].copy()\n",
    "acs = acs[acs['GEOID'].str.startswith(state_fips)].copy()\n",
    "print('Filtered tracts:', tracts.shape, 'Filtered ACS:', acs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b2e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building off of ph_computations_unweighted from  https://github.com/deniseg20/MSRIUP_TDA_Healthcare_Accessibility - ph_computations_unweighted.ipynb cell 10\n",
    "import geopandas as gpd\n",
    "\n",
    "# Census tract shapefile (national)\n",
    "shpfilename = \"https://www2.census.gov/geo/tiger/TIGER2020/TRACT/tl_2020_us_tract.zip\"\n",
    "tracts = gpd.read_file(shpfilename)\n",
    "\n",
    "# filter for NJ\n",
    "nj = tracts[tracts[\"STATEFP\"] == \"34\"]\n",
    "\n",
    "# reproject\n",
    "nj = nj.to_crs(\"EPSG:4326\")\n",
    "\n",
    "# load your processed ACS dataset\n",
    "gdf_vars = gpd.GeoDataFrame(acs, geometry=None) \n",
    "\n",
    "# Merge shapefile + attributes\n",
    "nj_merged = nj.merge(gdf_vars, left_on=\"GEOID\", right_on=\"FIPS\", how=\"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30960476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) merging datasets: tracts + ACS + FEA (via county FIPS)\n",
    "print('Merging datasets...')\n",
    "# FEA county fips detection\n",
    "fea_cols_lower = [c.lower() for c in fea.columns]\n",
    "county_fips_col = None\n",
    "for cand in ('countyfips','county_fips','fips','stcofips'):\n",
    "    if cand in fea_cols_lower:\n",
    "        county_fips_col = fea.columns[fea_cols_lower.index(cand)]\n",
    "        break\n",
    "if county_fips_col is None:\n",
    "    raise ValueError('Could not find county FIPS in FEA CSV.')\n",
    "\n",
    "fea[county_fips_col] = fea[county_fips_col].astype(str).str.zfill(5)\n",
    "\n",
    "# merging tracts with ACS by GEOID\n",
    "merged = tracts.merge(acs, on='GEOID', how='left', suffixes=('','_acs'))\n",
    "\n",
    "# attaching FEA by county\n",
    "merged['COUNTYFIPS'] = merged['GEOID'].str[:5]\n",
    "merged = merged.merge(fea, left_on='COUNTYFIPS', right_on=county_fips_col, how='left', suffixes=('','_fea'))\n",
    "\n",
    "print('Merged shape:', merged.shape)\n",
    "merged.to_csv(os.path.join(OUTPUT_DIR,'merged_initial.csv'), index=False)\n",
    "print('Saved merged_initial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44895180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) compute population density from ACS + tract shapefile area (ALAND) from ACS or shapefile\n",
    "# chosen approach: using ACS population (B01003 or similar) and 'ALAND' from shapefile. If ALAND is not in ACS, read the shapefile.\n",
    "print('Computing population density...')\n",
    "# find population column in ACS\n",
    "pop_col = next((c for c in acs.columns if c.lower().startswith('b01003') or 'population' in c.lower() or 'total'==c.lower()), None)\n",
    "if pop_col is None:\n",
    "    pop_col = next((c for c in acs.columns if 'population' in c.lower()), None)\n",
    "\n",
    "if pop_col is None:\n",
    "    raise ValueError('Could not detect a population column in ACS CSV (e.g., B01003).')\n",
    "\n",
    "merged['population'] = pd.to_numeric(merged[pop_col].fillna(0), errors='coerce')\n",
    "\n",
    "# get land area (from shapefile if ACS lacks ALAND)\n",
    "if 'ALAND' in merged.columns:\n",
    "    merged['ALAND'] = pd.to_numeric(merged['ALAND'], errors='coerce')\n",
    "else:\n",
    "    if os.path.exists(GEOJSON_PATH):\n",
    "        gdf = gpd.read_file(GEOJSON_PATH)\n",
    "    elif os.path.exists(TIGER_SHP):\n",
    "        gdf = gpd.read_file(TIGER_SHP)\n",
    "    else:\n",
    "        gdf = None\n",
    "\n",
    "    if gdf is not None:\n",
    "        if 'GEOID' not in gdf.columns:\n",
    "            gdf['GEOID'] = gdf['STATEFP'] + gdf['COUNTYFP'] + gdf['TRACTCE']\n",
    "        gdf = gdf[['GEOID','geometry']].copy()\n",
    "        # ensure projections for area calculation\n",
    "        gdf = gdf.to_crs(epsg=3857)\n",
    "        gdf['ALAND'] = gdf['geometry'].area  # area in m^2\n",
    "        # merge ALAND back to merged\n",
    "        merged = merged.merge(gdf[['GEOID','ALAND']], on='GEOID', how='left')\n",
    "    else:\n",
    "        raise ValueError('No shapefile or geojson available to compute land area. Provide TIGER shapefile or GeoJSON.')\n",
    "\n",
    "# convert ALAND (m^2) to km^2\n",
    "merged['ALAND_km2'] = pd.to_numeric(merged['ALAND'], errors='coerce') / 1e6\n",
    "merged['density'] = merged['population'] / merged['ALAND_km2']\n",
    "merged['density'] = merged['density'].replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "\n",
    "print('Density computed. Summary:')\n",
    "print(merged['density'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df06f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) select poverty and income fields\n",
    "# heuristics: find columns with 'poverty' and 'income'\n",
    "poverty_col = next((c for c in merged.columns if 'poverty' in c.lower()), None)\n",
    "income_col = next((c for c in merged.columns if 'income' in c.lower() and 'median' in c.lower()), None)\n",
    "\n",
    "if poverty_col is None:\n",
    "    # ACS standard fields (e.g., B17001 or S1701) \n",
    "    poverty_col = next((c for c in merged.columns if 'b17001' in c.lower() or 'poverty' in c.lower()), None)\n",
    "\n",
    "if income_col is None:\n",
    "    income_col = next((c for c in merged.columns if 'median' in c.lower() and 'income' in c.lower()), None)\n",
    "\n",
    "print('Detected poverty col:', poverty_col, 'Detected income col:', income_col)\n",
    "\n",
    "# coerce to numeric and fillna\n",
    "merged['poverty_rate'] = pd.to_numeric(merged[poverty_col], errors='coerce') if poverty_col else 0\n",
    "merged['median_income'] = pd.to_numeric(merged[income_col], errors='coerce') if income_col else np.nan\n",
    "merged['poverty_rate'] = merged['poverty_rate'].fillna(merged['poverty_rate'].median())\n",
    "merged['median_income'] = merged['median_income'].fillna(merged['median_income'].median())\n",
    "\n",
    "# standardize socioeconomic variables before weighting to prevent imbalance\n",
    "scaler = StandardScaler()\n",
    "scaler_cols = ['poverty_rate','median_income','density']\n",
    "merged[['poverty_s','income_s','density_s']] = scaler.fit_transform(merged[scaler_cols].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc6d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) compute geographic centroids and pairwise geographic distances (km)\n",
    "print('Computing tract centroids and geographic pairwise distances...')\n",
    "if 'geometry' not in locals():\n",
    "    # load shapefile/geojson again to obtain geometry if not present\n",
    "    if os.path.exists(GEOJSON_PATH):\n",
    "        gdf = gpd.read_file(GEOJSON_PATH)\n",
    "    else:\n",
    "        gdf = gpd.read_file(TIGER_SHP)\n",
    "\n",
    "    if 'GEOID' not in gdf.columns:\n",
    "        gdf['GEOID'] = gdf['STATEFP'] + gdf['COUNTYFP'] + gdf['TRACTCE']\n",
    "\n",
    "    gdf = gdf[['GEOID','geometry']]\n",
    "\n",
    "# ensure same CRS for centroid calculation\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "gdf['centroid'] = gdf.geometry.centroid\n",
    "\n",
    "# merge centroids onto merged table\n",
    "gdf_cent = gdf[['GEOID','centroid']].copy()\n",
    "merged = merged.merge(gdf_cent, on='GEOID', how='left')\n",
    "merged['centroid'] = merged['centroid'].apply(lambda x: x if not pd.isna(x) else None)\n",
    "\n",
    "# build numpy array of lon/lat in EPSG:4326 for distances\n",
    "# convert centroids to lat/lon\n",
    "gdf_latlon = gdf.copy().to_crs(epsg=4326)\n",
    "coords = np.array([[pt.y, pt.x] for pt in gdf_latlon['centroid']])  # lat, lon\n",
    "\n",
    "# map GEOIDs to row order in merged\n",
    "g_order = merged['GEOID'].tolist()\n",
    "# create mapping from GEOID to centroid (lat,lon)\n",
    "geo_to_latlon = {g: (np.nan, np.nan) for g in g_order}\n",
    "for idx, row in gdf_latlon.iterrows():\n",
    "    geo_to_latlon[row['GEOID']] = (row['centroid'].y, row['centroid'].x)\n",
    "\n",
    "coords_ordered = np.array([geo_to_latlon[g] for g in g_order])\n",
    "\n",
    "# compute haversine distances (km)\n",
    "def haversine_array(latlon):\n",
    "    # latlon: (n,2) with (lat, lon) in degrees\n",
    "    lat = np.radians(latlon[:,0])\n",
    "    lon = np.radians(latlon[:,1])\n",
    "    dlat = lat[:,None] - lat[None,:]\n",
    "    dlon = lon[:,None] - lon[None,:]\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat)[:,None]*np.cos(lat)[None,:]*np.sin(dlon/2)**2\n",
    "    R = 6371.0\n",
    "    d = 2*R*np.arcsin(np.sqrt(a))\n",
    "    return d\n",
    "\n",
    "D_geo = haversine_array(coords_ordered)  # (n,n) in km\n",
    "print('Geographic distance matrix computed; shape:', D_geo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa218d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) build weighted distance matrix\n",
    "print('Building weighted distance matrix (Option 2)...')\n",
    "\n",
    "poverty_s = merged['poverty_s'].to_numpy(dtype=float)\n",
    "income_s = merged['income_s'].to_numpy(dtype=float)\n",
    "density_s = merged['density_s'].to_numpy(dtype=float)\n",
    "\n",
    "# pairwise absolute differences\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "D_pov = squareform(pdist(poverty_s.reshape(-1,1), metric='cityblock'))\n",
    "D_inc = squareform(pdist(income_s.reshape(-1,1), metric='cityblock'))\n",
    "D_den = squareform(pdist(density_s.reshape(-1,1), metric='cityblock'))\n",
    "\n",
    "# distance metric - \n",
    "D_weighted = D_geo + alpha_poverty * D_pov + alpha_income * D_inc + alpha_density * D_den\n",
    "\n",
    "# Normalize or scale if needed (optional). We'll set MAX_EDGE to 90th percentile to limit complex size\n",
    "MAX_EDGE = np.percentile(D_weighted[np.triu_indices_from(D_weighted, k=1)], 90)\n",
    "print('Weighted distance matrix built. MAX_EDGE set to 90th percentile:', MAX_EDGE)\n",
    "\n",
    "# Save small sample for reproducibility\n",
    "np.save(os.path.join(OUTPUT_DIR,'D_weighted.npy'), D_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 11) build VR complex from distance matrix and compute persistence\n",
    "if not USE_GUDHI:\n",
    "    raise RuntimeError('GUDHI is not available. Install GUDHI to continue.')\n",
    "\n",
    "print('Creating GUDHI RipsComplex from distance matrix...')\n",
    "start = time.time()\n",
    "rips = gd.RipsComplex(distance_matrix=D_weighted, max_edge_length=MAX_EDGE)\n",
    "simplicial_tree = rips.create_simplex_tree(max_dimension=MAX_DIM)\n",
    "print('Number of simplices:', simplicial_tree.num_simplices())\n",
    "print('Computing persistence...')\n",
    "pers = simplicial_tree.persistence()\n",
    "print('Persistence computed in %.2f s' % (time.time()-start))\n",
    "\n",
    "# plot persistence diagram\n",
    "gd.plot_persistence_diagram(pers)\n",
    "plt.title('GUDHI: Weighted VR persistence (H0,H1)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cbea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) extract persistence features and death simplices\n",
    "print('Extracting persistence features (global) and death simplices...')\n",
    "# convert to diagram array for H0/H1 separation - to view in different homology classes\n",
    "diag = simplicial_tree.persistence_intervals_in_dimension\n",
    "\n",
    "# helper to extract diagrams per dimension\n",
    "def get_diagram(stree, dim):\n",
    "    return np.array([pt[1] for pt in stree.persistence() if stree.persistence()[0] is not None])\n",
    "\n",
    "# simplicial_tree.persistence() returns persistence as list of (dim, (birth, death)) \n",
    "# iterate for H0 (0-dim homology class) and H1 (1-dim homology class)\n",
    "h0 = []\n",
    "h1 = []\n",
    "for d in pers:\n",
    "    dim = d[0]\n",
    "    b, de = d[1]\n",
    "    if de == float('inf'):\n",
    "        de = MAX_EDGE*1.1\n",
    "    if dim == 0:\n",
    "        h0.append((b,de))\n",
    "    elif dim == 1:\n",
    "        h1.append((b,de))\n",
    "\n",
    "h0 = np.array(h0) if len(h0)>0 else np.empty((0,2))\n",
    "h1 = np.array(h1) if len(h1)>0 else np.empty((0,2))\n",
    "\n",
    "# persistence stats (global)\n",
    "def pers_stats(dgm):\n",
    "    if dgm.size==0:\n",
    "        return {'n':0,'mean':0,'max':0,'tot':0}\n",
    "    pers = dgm[:,1] - dgm[:,0]\n",
    "    return {'n':len(pers), 'mean':np.mean(pers), 'max':np.max(pers), 'tot':np.sum(pers)}\n",
    "\n",
    "stats_h0 = pers_stats(h0)\n",
    "stats_h1 = pers_stats(h1)\n",
    "print('H0 stats:', stats_h0)\n",
    "print('H1 stats:', stats_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ML we want per-tract features. compute local per-tract diagrams using neighborhoods (knn) then compute stats per tract.\n",
    "# 13) compute per-tract local diagrams using knn neighborhoods on feature space (PCA of socioeconomic features + coords)\n",
    "print('computing local per-tract diagrams via knn neighborhoods...')\n",
    "# Build per-tract feature vectors for neighbor search: use centroids + standardized socioeconomic features\n",
    "feat_arr = np.vstack([coords_ordered[:,0], coords_ordered[:,1], merged['poverty_s'].to_numpy(), merged['income_s'].to_numpy(), merged['density_s'].to_numpy()]).T\n",
    "\n",
    "# optional dim reduction\n",
    "pca = PCA(n_components=min(8, feat_arr.shape[1]))\n",
    "feat_pca = pca.fit_transform(feat_arr)\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=min(K_NEIGHBORS, feat_pca.shape[0]-1)).fit(feat_pca)\n",
    "neighbors = nn.kneighbors(return_distance=False)\n",
    "\n",
    "local_feature_list = []\n",
    "for i in range(feat_pca.shape[0]):\n",
    "    idxs = neighbors[i]\n",
    "    local_pts = feat_pca[idxs]\n",
    "    # compute local weighted distance using same formula but on the subset\n",
    "    # compute geo distances on subset centroids\n",
    "    sub_geo = D_geo[np.ix_(idxs, idxs)]\n",
    "    sub_pov = squareform(pdist(merged['poverty_s'].to_numpy()[idxs].reshape(-1,1), metric='cityblock'))\n",
    "    sub_inc = squareform(pdist(merged['income_s'].to_numpy()[idxs].reshape(-1,1), metric='cityblock'))\n",
    "    sub_den = squareform(pdist(merged['density_s'].to_numpy()[idxs].reshape(-1,1), metric='cityblock'))\n",
    "    D_sub = sub_geo + alpha_poverty*sub_pov + alpha_income*sub_inc + alpha_density*sub_den\n",
    "    \n",
    "    # build vr complexes\n",
    "    rips_sub = gd.RipsComplex(distance_matrix=D_sub, max_edge_length=np.percentile(D_sub[np.triu_indices_from(D_sub, k=1)],90))\n",
    "    st_sub = rips_sub.create_simplex_tree(max_dimension=1)\n",
    "    pers_sub = st_sub.persistence()\n",
    "    \n",
    "    # collect H1 stats\n",
    "    h1_sub = [d[1] for d in pers_sub if d[0]==1]\n",
    "    if len(h1_sub)==0:\n",
    "        local_stats = [0,0,0]\n",
    "    else:\n",
    "        arr = np.array(h1_sub)\n",
    "        pers_vals = arr[:,1] - arr[:,0]\n",
    "        local_stats = [len(pers_vals), float(np.mean(pers_vals)), float(np.max(pers_vals))]\n",
    "    local_feature_list.append(local_stats)\n",
    "\n",
    "local_feat_df = pd.DataFrame(local_feature_list, columns=['h1_n','h1_mean','h1_max'])\n",
    "local_feat_df['GEOID'] = merged['GEOID'].values\n",
    "local_feat_df.to_csv(os.path.join(OUTPUT_DIR,'gudhi_local_persistence_features.csv'), index=False)\n",
    "print('Saved per-tract local features; shape:', local_feat_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f0a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) build ML dataset and run sample classifier\n",
    "print('Building ML dataset using per-tract persistence stats...')\n",
    "ml_df = merged.reset_index(drop=True).merge(local_feat_df, on='GEOID', how='left')\n",
    "ml_df[['h1_n','h1_mean','h1_max']] = ml_df[['h1_n','h1_mean','h1_max']].fillna(0)\n",
    "\n",
    "# create target: low median income (below state median)\n",
    "if 'median_income' in ml_df.columns:\n",
    "    ml_df['target_low_income'] = (ml_df['median_income'] < ml_df['median_income'].median()).astype(int)\n",
    "    y = ml_df['target_low_income'].values\n",
    "else:\n",
    "    y = None\n",
    "\n",
    "feature_cols = ['h1_n','h1_mean','h1_max','poverty_s','income_s','density_s']\n",
    "X = ml_df[feature_cols].astype(float).fillna(0).values\n",
    "\n",
    "if y is not None:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "    acc = clf.score(X_test, y_test)\n",
    "    cv = cross_val_score(clf, X, y, cv=5).mean()\n",
    "    print('RandomForest test accuracy:', acc)\n",
    "    print('5-fold CV accuracy:', cv)\n",
    "    \n",
    "    # feature importances\n",
    "    fi = pd.Series(clf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "    print('Feature importances:')\n",
    "    print(fi)\n",
    "else:\n",
    "    print('No target column detected; provide median_income or another target for supervised tasks.')\n",
    "\n",
    "# saving ML dataset\n",
    "ml_df.to_csv(os.path.join(OUTPUT_DIR,'ml_dataset_gudhi.csv'), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb47571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) mapping: choropleth of H1 mean persistence and poverty\n",
    "print('Mapping: creating choropleth (H1 mean persistence and poverty)')\n",
    "# ensure geometry in gdf (loaded earlier)\n",
    "if 'gdf' not in locals():\n",
    "    if os.path.exists(GEOJSON_PATH):\n",
    "        gdf = gpd.read_file(GEOJSON_PATH)\n",
    "    else:\n",
    "        gdf = gpd.read_file(TIGER_SHP)\n",
    "\n",
    "if 'GEOID' not in gdf.columns:\n",
    "    gdf['GEOID'] = gdf['STATEFP'] + gdf['COUNTYFP'] + gdf['TRACTCE']\n",
    "\n",
    "gdf = gdf.merge(local_feat_df, on='GEOID', how='left')\n",
    "\n",
    "gdf = gdf.to_crs(epsg=3857)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(16,8))\n",
    "gdf.plot(column='h1_mean', ax=axes[0], legend=True, cmap='OrRd', edgecolor='0.2', linewidth=0.1)\n",
    "axes[0].set_title('Per-tract H1 mean persistence (GUDHI local k-NN)')\n",
    "\n",
    "if 'poverty_rate' in merged.columns:\n",
    "    # join poverty_rate to gdf\n",
    "    pov_df = merged[['GEOID','poverty_rate']]\n",
    "    gdf = gdf.merge(pov_df, on='GEOID', how='left')\n",
    "    gdf.plot(column='poverty_rate', ax=axes[1], legend=True, cmap='YlGnBu', edgecolor='0.2', linewidth=0.1)\n",
    "    axes[1].set_title('Poverty rate')\n",
    "else:\n",
    "    axes[1].axis('off')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving our results\n",
    "print('Saving artifacts...')\n",
    "np.save(os.path.join(OUTPUT_DIR,'D_weighted.npy'), D_weighted)\n",
    "with open(os.path.join(OUTPUT_DIR,'gudhi_persistence_global.json'),'w') as f:\n",
    "    import json\n",
    "    json.dump([(int(d[0]), [float(d[1][0]), float(d[1][1]) if d[1][1]!=float('inf') else None]) for d in pers], f)\n",
    "\n",
    "print('All artifacts saved to', OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
